The topic of the seminar is **Investigating Entity Knowledge in BERT with Simple Neural End-To-End Entity Linking**. This work justifies how End-to-End-End Entity Linking is beneficial in training a model and investigates how additional entity knowledge improves performance by fine-tuning **BERT** language model.

Pre-requisities:
- Sequence to sequence models
- Recurrent Neural Networks
- Transformer models
- BERT language model

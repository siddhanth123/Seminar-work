The topic of the seminar is **Investigating Entity Knowledge in BERT with Simple Neural End-To-End Entity Linking**. This work justifies how End-to-End-End Entity Linking is beneficial in training a model and investigates how additional entity knowledge improves the performance of fine-tuned **BERT** language model.

Pre-requisities:
- Natural Language Processing
- Sequence to sequence models
- Recurrent Neural Networks
- Transformer models
- BERT language model
